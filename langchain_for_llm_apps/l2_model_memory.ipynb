{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Memory\n",
    "### Outline\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = Ollama(\n",
    "    model = \"llama3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "converation = ConversationChain(llm = llm, memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Iqbal! It's great to chat with you. I'm an AI designed to assist and communicate with humans in a helpful and informative way. My training data consists of a massive corpus of text from various sources, including books, articles, research papers, and more. According to my knowledge cutoff in 2022, the world population is approximately 7.92 billion people. What would you like to talk about today?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converation.predict(input = \"Hi This is Iqbal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A simple yet great question to start our conversation! According to my mathematical training data, 1 + 1 equals 2. In fact, this fundamental arithmetic operation is a basic building block of mathematics and is used in countless calculations across various disciplines, from physics to economics. Would you like to explore more mathematical concepts or move on to something else?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converation.predict(input = \"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A great question about global politics! As of my knowledge cutoff in 2022, the President of India is Droupadi Murmu. She is the 28th Vice President and the first female Vice President of India, serving since August 2022. Prior to her current role, she served as a Member of Parliament (MP) from May 2016 to August 2022. As a ceremonial head of state, the President of India plays a crucial symbolic role in Indian politics, with most executive powers vested in the Prime Minister. Would you like to know more about Indian politics or move on to another topic?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converation.predict(input = \"Who is the president of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A question that's easy to answer! According to our conversation just started, your name is Iqbal. You introduced yourself as such at the beginning of our chat. Is there anything else you'd like to talk about or ask about? I'm here to help and provide information on a wide range of topics!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converation.predict(input = \"What is my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi This is Iqbal\n",
      "AI: Hello Iqbal! It's great to chat with you. I'm an AI designed to assist and communicate with humans in a helpful and informative way. My training data consists of a massive corpus of text from various sources, including books, articles, research papers, and more. According to my knowledge cutoff in 2022, the world population is approximately 7.92 billion people. What would you like to talk about today?\n",
      "Human: What is 1+1?\n",
      "AI: A simple yet great question to start our conversation! According to my mathematical training data, 1 + 1 equals 2. In fact, this fundamental arithmetic operation is a basic building block of mathematics and is used in countless calculations across various disciplines, from physics to economics. Would you like to explore more mathematical concepts or move on to something else?\n",
      "Human: Who is the president of India?\n",
      "AI: A great question about global politics! As of my knowledge cutoff in 2022, the President of India is Droupadi Murmu. She is the 28th Vice President and the first female Vice President of India, serving since August 2022. Prior to her current role, she served as a Member of Parliament (MP) from May 2016 to August 2022. As a ceremonial head of state, the President of India plays a crucial symbolic role in Indian politics, with most executive powers vested in the Prime Minister. Would you like to know more about Indian politics or move on to another topic?\n",
      "Human: What is my name\n",
      "AI: A question that's easy to answer! According to our conversation just started, your name is Iqbal. You introduced yourself as such at the beginning of our chat. Is there anything else you'd like to talk about or ask about? I'm here to help and provide information on a wide range of topics!\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi This is Iqbal\\nAI: Hello Iqbal! It's great to chat with you. I'm an AI designed to assist and communicate with humans in a helpful and informative way. My training data consists of a massive corpus of text from various sources, including books, articles, research papers, and more. According to my knowledge cutoff in 2022, the world population is approximately 7.92 billion people. What would you like to talk about today?\\nHuman: What is 1+1?\\nAI: A simple yet great question to start our conversation! According to my mathematical training data, 1 + 1 equals 2. In fact, this fundamental arithmetic operation is a basic building block of mathematics and is used in countless calculations across various disciplines, from physics to economics. Would you like to explore more mathematical concepts or move on to something else?\\nHuman: Who is the president of India?\\nAI: A great question about global politics! As of my knowledge cutoff in 2022, the President of India is Droupadi Murmu. She is the 28th Vice President and the first female Vice President of India, serving since August 2022. Prior to her current role, she served as a Member of Parliament (MP) from May 2016 to August 2022. As a ceremonial head of state, the President of India plays a crucial symbolic role in Indian politics, with most executive powers vested in the Prime Minister. Would you like to know more about Indian politics or move on to another topic?\\nHuman: What is my name\\nAI: A question that's easy to answer! According to our conversation just started, your name is Iqbal. You introduced yourself as such at the beginning of our chat. Is there anything else you'd like to talk about or ask about? I'm here to help and provide information on a wide range of topics!\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Hi\"},{\"output\":\"This is Dog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: This is Dog\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi\\nAI: This is Dog'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi\\nAI: This is Dog\\nHuman: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "converation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: I am Iqbal\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Iqbal! I'm an AI designed to assist and converse with humans like yourself. My knowledge base is derived from a massive corpus of text data, which I use to generate responses to your queries. Speaking of which, what brings you here today? Would you like to talk about something specific or just see where the conversation takes us?\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converation.predict(input = \"I am Iqbal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: I am Iqbal\n",
      "AI: Nice to meet you, Iqbal! I'm an AI designed to assist and converse with humans like yourself. My knowledge base is derived from a massive corpus of text data, which I use to generate responses to your queries. Speaking of which, what brings you here today? Would you like to talk about something specific or just see where the conversation takes us?\n",
      "Human: 21 is the 15%  of?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Iqbal! That's an interesting question. After searching my vast knowledge base, I found that 21 is approximately 15% of 140. This is because 21 is a prime number and a common multiple in many mathematical contexts. For instance, it's the sum of the first five consecutive integers (1+2+3+4+5), which makes it an important value in number theory.\\n\\nIf you'd like to explore more about this topic or move on to something else, I'm all ears!\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converation.predict(input = \"21 is the 15%  of?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 21 is the 15%  of?\n",
      "AI: Iqbal! That's an interesting question. After searching my vast knowledge base, I found that 21 is approximately 15% of 140. This is because 21 is a prime number and a common multiple in many mathematical contexts. For instance, it's the sum of the first five consecutive integers (1+2+3+4+5), which makes it an important value in number theory.\n",
      "\n",
      "If you'd like to explore more about this topic or move on to something else, I'm all ears!\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm glad you asked! Unfortunately, I don't have that information. As a conversational AI, I don't have access to personal details about individual humans, including their names. My training data only provides general knowledge and doesn't contain specific information about individuals. So, I won't be able to tell you your name or any other personal details. If you'd like to chat about something else, I'm happy to help!\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converation.predict(input = \"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: Current summary:\\n\\n(No text provided)\\n\\nNew lines of conversation:\\n\\nHuman: Hello\\nAI: What\\'s up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\n\\nNew summary:\\n\\nThe conversation has just begun. The human says \"Hello\" and the AI responds with \"What\\'s up\". The human mentions they\\'re not doing much and the AI responds with \"Cool\". Finally, the human asks what is scheduled for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: Current summary:\n",
      "\n",
      "(No text provided)\n",
      "\n",
      "New lines of conversation:\n",
      "\n",
      "Human: Hello\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is on the schedule today?\n",
      "\n",
      "New summary:\n",
      "\n",
      "The conversation has just begun. The human says \"Hello\" and the AI responds with \"What's up\". The human mentions they're not doing much and the AI responds with \"Cool\". Finally, the human asks what is scheduled for the day.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I can suggest some ideas for a compelling demo! Since the customer is interested in understanding the latest developments in AI, I think showing something that highlights our language model's capabilities would be impressive.\\n\\nOne idea could be to demonstrate how our LLM can generate coherent and context-specific text based on a prompt. You could show an example of asking the model to summarize a long piece of text or generating a short story based on a theme. The customer might be impressed by the model's ability to understand nuances in language and create human-like responses.\\n\\nAlternatively, you could showcase our LLM's ability to answer complex questions, providing detailed explanations and examples. This could be particularly impressive if you can demonstrate how the model can handle multiple steps or follow-up questions.\\n\\nOf course, these are just a few ideas, but I'm sure there are many other creative ways to show off our LLM's capabilities! What do you think?\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: Current summary:\\nThe conversation has just begun. The human says \"Hello\" and the AI responds with \"What\\'s up\". The human mentions they\\'re not doing much and the AI responds with \"Cool\". Finally, the human asks what is scheduled for the day.\\n\\nNew lines of conversation:\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\\nHuman: What would be a good demo to show?\\nAI: I can suggest some ideas for a compelling demo! Since the customer is interested in understanding the latest developments in AI, I think showing something that highlights our language model\\'s capabilities would be impressive. One idea could be to demonstrate how our LLM can generate coherent and context-specific text based on a prompt. You could show an example of asking the model to summarize a long piece of text or generating a short story based on a theme. The customer might be impressed by the model\\'s ability to understand nuances in language and create human-like responses. Alternatively, you could showcase our LLM\\'s ability to answer complex questions, providing detailed explanations and examples. This could be particularly impressive if you can demonstrate how the model can handle multiple steps or follow-up questions. Of course, these are just a few ideas, but I\\'m sure there are many other creative ways to show off our LLM\\'s capabilities! What do you think?\\n\\nNew summary:\\nThe conversation has just begun. The human says \"Hello\" and the AI responds with \"What\\'s up\". The human mentions they\\'re not doing much and the AI responds with \"Cool\". Finally, the human asks what is scheduled for the day. The AI then provides a schedule for the day, including a meeting at 8am, work on a LangChain project, and lunch at an Italian restaurant to meet with a customer interested in understanding the latest developments in AI. The human asks for ideas on what to demonstrate during the LLM demo, and the AI suggests showing off its language model\\'s capabilities by generating text based on a prompt or answering complex questions.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
